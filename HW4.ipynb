{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cUMSpya-I1hC"
      },
      "source": [
        "# Week 04 seminar: Finetuning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "OaiT_Hx-I1hD"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "import requests\n",
        "from tqdm.auto import trange, tqdm\n",
        "from copy import deepcopy\n",
        "from collections import defaultdict\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# charts and display libs\n",
        "from PIL import Image, ImageFile\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "# pytorch\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Ensure that output below says `device=device(type='cuda')` - you will need CUDA for faster model runs\n",
        "# If in Colab, we recommend that you go to Runtime -> Change Runtime Type -> GPU\n",
        "\n",
        "print(f\"{torch.__version__=}, {torchvision.__version__=}, {device=}, {torch.get_num_threads()=}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iYytoXOKibf0",
        "outputId": "8d15a795-b6ec-41fa-f2e7-7ced32d9ce90"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I354gMY9I1hE"
      },
      "source": [
        "## TorchVision\n",
        "\n",
        "[Torchvision](https://pytorch.org/vision/main/index.html) - part of PyTorch library with convenient tools and data for deep learning in visual domain.\n",
        "- contains a number of popular vision [datasets](https://pytorch.org/vision/stable/datasets.html)\n",
        "- preprocessing [tools](https://pytorch.org/vision/stable/transforms.html)\n",
        "- and most importantly, [pre-trained models](https://pytorch.org/vision/main/models.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xrA6NNZpI1hF"
      },
      "source": [
        "# Datasets: Imagenet\n",
        "\n",
        "![imagenet_tiles](https://i.imgur.com/n4QIrzF.jpeg)\n",
        "\n",
        "Today we're going to use and fine-tune CNN based on weights pre-trained on ImageNet.\n",
        "\n",
        "What is Imagenet?\n",
        "- large size image classification dataset.\n",
        "    - ImageNet-1K contains 1,281,167 training images, 50,000 validation images and 100,000 test images.\n",
        "    - Full original dataset (ImageNet-21k) contains 14,197,122 images divided into 21,841 classes\n",
        "    - Resolution varies, average resolution: 469x387 pixels\n",
        "- built pre-2010 by [Fei-Fei Li](https://en.wikipedia.org/wiki/Fei-Fei_Li) at Princeton\n",
        "- made very popular by ImageNet Large Scale Visual Recognition Challenge (ILSVRC). See AlexNet moment: [chart](https://www.researchgate.net/figure/ImageNet-Competition-Results-50_fig1_329975404), [wiki](https://en.wikipedia.org/wiki/AlexNet), [paper](https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf)\n",
        "-  still relevant; [accuracy history 2013 to date](https://paperswithcode.com/sota/image-classification-on-imagenet)\n",
        "- More about Imagenet: http://image-net.org/,  https://en.wikipedia.org/wiki/ImageNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "oNE4tlvpI1hF"
      },
      "outputs": [],
      "source": [
        "# loading Imagenet class labels for interpreting classification results\n",
        "LABELS_URL = \"https://s3.amazonaws.com/deep-learning-models/image-models/imagenet_class_index.json\"\n",
        "imagenet_labels = {int(k):v[1] for k, v in requests.get(LABELS_URL).json().items()}\n",
        "print(len(imagenet_labels), '\\n', list(imagenet_labels.items())[:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "VKFvvRMfI1hF"
      },
      "source": [
        "# Pretrained models: Resnet\n",
        "\n",
        "\n",
        "Torchvision models classification models with benchmarks may be viewed [here.](https://pytorch.org/vision/main/models.html#classification)\n",
        "\n",
        "For this seminar we're going to use Pytorch implementation of popular Resnet model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "tags": [],
        "id": "TRX47BKTI1hF"
      },
      "outputs": [],
      "source": [
        "# loading pretrained Resnet-18 model\n",
        "from torchvision.models import resnet18, ResNet18_Weights\n",
        "\n",
        "model = resnet18(weights=ResNet18_Weights.DEFAULT) # load model with best available weights\n",
        "model = model.to(device)  # move the model to GPU if available\n",
        "model.train(False);        # set the model to evaluation mode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "fouatnIKI1hF"
      },
      "outputs": [],
      "source": [
        "# view the model structure. Familiar layers are combined into 4 blocks\n",
        "# note the last LINEAR layer named 'fc' that converts embeddings of size 512 into logits for 1000 Imagenet classes\n",
        "model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "wLtoXVKII1hF"
      },
      "source": [
        "### testing the pretrained model 1\n",
        "test output dimensions with dummy inputs<br>\n",
        "note that model inputs have to be 4D: (batch_size, color_channes, height, width)<br>\n",
        "output is a 2D tensor of logits (batch_size, number_of_classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "HenDxs9QI1hG"
      },
      "outputs": [],
      "source": [
        "dummy_x = torch.randn(5, 3, 224, 224, device=device)  # dummy batch of 5 'images' sized 224x224 with 3 channels, created on GPU\n",
        "result = model(dummy_x)\n",
        "assert result.shape == (5, 1000)   # output is a 2D tensor of logits (batch_size, number_of_classes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "TSnXZ8QFI1hG"
      },
      "source": [
        "### testing the pretrained model 2. Predict class probabilities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "UrUigfm1I1hG"
      },
      "outputs": [],
      "source": [
        "# loading image with PIL library\n",
        "img = Image.open(requests.get('https://github.com/yandexdataschool/Practical_DL/blob/fall24/week04_finetuning/sample_images/albatross.jpg?raw=true', stream=True).raw)\n",
        "# img = Image.open('sample_images/albatross.jpg')  # alternative source\n",
        "print(img.size)\n",
        "img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "z1xMeqwEI1hG"
      },
      "outputs": [],
      "source": [
        "# converting PIL image to torch.Tensor - detailed process\n",
        "img_torch = torch.tensor(np.array(img), device=device)  # convert PIL image to np.array, then to torch.Tensor,\n",
        "img_torch = img_torch.permute(2,0,1)  # reorder channels to move color to the front position, to match pytorch conventions\n",
        "img_torch = img_torch / img_torch.max()  # scale to 0..1\n",
        "\n",
        "img_torch.shape, img_torch.device, img_torch.min().item(), img_torch.max().item()  # verify shape, device and range"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "7jGLnSgdI1hG"
      },
      "outputs": [],
      "source": [
        "# converting PIL image to torch.Tensor - easy process\n",
        "# PIL image to torch.Tensor can be converted with torchvision.transforms, equivalent to the above code (more details below)\n",
        "img_torch = transforms.ToTensor()(img)\n",
        "img_torch.shape, img_torch.max(), img_torch.min()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "vthf1tOUI1hG"
      },
      "outputs": [],
      "source": [
        "# Predicting image class with pretrained model\n",
        "\n",
        "\n",
        "def predict_img(img, model, top_n=5):\n",
        "    if isinstance(img, str):\n",
        "        Image.open(requests.get(url, stream=True).raw).convert('RGB')  # for loading images from url\n",
        "\n",
        "    img_torch = transforms.ToTensor()(img)  # to torch.Tensor, reorder color channels, s|cale to 0..1\n",
        "    img_torch = transforms.Resize((224, 224))(img_torch)      # another useful transform to resize images\n",
        "    img_torch = img_torch.unsqueeze(0)                        # add batch dimension (remember that model needs 4D input)\n",
        "    img_torch = img_torch.to(device)                          # moving the tensor to device (presumably cuda, in initialized above)\n",
        "    prediction = model(img_torch)                             # obtain prediction logits from the model\n",
        "    probs = torch.nn.functional.softmax(prediction, dim=-1)   # convert logits into probabilities\n",
        "    probs = probs.cpu().data.numpy()                          # convert CUDA tensor to numpy array\n",
        "\n",
        "    top_ix = probs.ravel().argsort()[-1: -top_n - 1: -1]      # get indices of most probable classes\n",
        "    print (f'top-{top_n} classes:')                           # look up class label\n",
        "    for l in top_ix:\n",
        "        print (f\"{probs.ravel()[l]:>6.2%}  {imagenet_labels[l].split(',')[0]}\")\n",
        "\n",
        "\n",
        "predict_img(img, model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M38KOiCpI1hH"
      },
      "source": [
        "### testing the pretrained model 3: Images from unknown classes\n",
        "\n",
        "Play with imahes from these and other (yours) URLS. Note how object outsize of imagenet classes confuse the net. Low probabilities in top classes are indications of model's low confidence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "UV37owaEI1hH"
      },
      "outputs": [],
      "source": [
        "url= 'https://i.ebayimg.com/images/g/yDwAAOSwquxgTmcv/s-l1200.jpg'                    # modern version\n",
        "\n",
        "# <TRY ANY OTHER IMAGES YOU LIKE>\n",
        "\n",
        "web_img = Image.open(requests.get(url, stream=True).raw).convert('RGB')\n",
        "print(web_img.size)\n",
        "display(transforms.Resize((224, 224))(web_img))\n",
        "predict_img(web_img, model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MAguGzVBI1hH"
      },
      "source": [
        "## More Torchvision tools: Transforms and transform pipelines¶\n",
        "\n",
        "You already used `transforms.ToTensor` and `transforms.Resize` above. There are many more at [Torchvision](https://pytorch.org/vision/stable/transforms.html). For easier application they are typically combined into pipelines. See examples below.\n",
        "\n",
        "For more advanced tranforms (faster and compatible with tasks requiring mask or reference points), check out [Albumentations library](https://albumentations.ai/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "4NjXqbAmI1hH"
      },
      "outputs": [],
      "source": [
        "# Typical transform pipeline for test loop\n",
        "transform_pipeline = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])  # Optional: normalize imaged according to ImageNet standards\n",
        "])\n",
        "\n",
        "img = Image.open(requests.get('https://github.com/yandexdataschool/Practical_DL/blob/fall24/week04_finetuning/sample_images/albatross.jpg?raw=true', stream=True).raw)\n",
        "# img = Image.open('sample_images/albatross.jpg')  # alt link\n",
        "img_torch = transform_pipeline(img)\n",
        "\n",
        "print(type(img_torch), img_torch.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "p8WqQpGgI1hH"
      },
      "outputs": [],
      "source": [
        "img = Image.open(requests.get('https://github.com/yandexdataschool/Practical_DL/blob/fall24/week04_finetuning/sample_images/albatross.jpg?raw=true', stream=True).raw)\n",
        "# img = Image.open('sample_images/albatross.jpg')  # alternative source\n",
        "\n",
        "# Demo of augmentations for train pipeline\n",
        "\n",
        "transform_pipeline_2 = transforms.Compose([\n",
        "    transforms.RandomCrop((224, 224)),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n",
        "    transforms.RandomApply([\n",
        "        transforms.RandomAffine(degrees=30, translate=(0.2, 0.2)),\n",
        "        transforms.RandomRotation(30),\n",
        "        transforms.GaussianBlur(kernel_size=25),\n",
        "    ], p=0.5),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    # transforms.RandomVerticalFlip(),  # not always applicable\n",
        "    transforms.ToTensor(),\n",
        "    # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  # Optional: normalizes imaged according to ImageNet standards\n",
        "])\n",
        "\n",
        "fig, axs = plt.subplots(1, 8, figsize=(16, 4))\n",
        "for i, ax in enumerate(axs.ravel()):\n",
        "    img_2 = transform_pipeline_2(img)\n",
        "    ax.imshow(img_2.permute(1, 2, 0))\n",
        "plt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4p3nzyG8I1hH"
      },
      "source": [
        "# Classifying with CNN model's latent features\n",
        "Pretrained image classification models learn extract image features that are useful in classification tasks. We need to get those features from outputs of the model's penultimate level and pass them to classifier.\n",
        "While this is not exactly a proper finetuning, this method is quick, rather robust and allows to classify unknown classes using quite small training sets (tens / hundreds of images)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iAhsPGRtI1hH"
      },
      "source": [
        "### How to get features\n",
        "features = activations before the very last Linear layer of the model (named `fc` in Resnet - check the model structure above.\n",
        "\n",
        "During good old days in Torch7 you could access any intermediate output from the sequential model. Nowadays it's a bit more difficult though it's not Tensorflow where you need to compile another model for that. Here we're going to redefine the last layer... yes, to do nothing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "IEgUpiR7I1hI"
      },
      "outputs": [],
      "source": [
        "# Create model clone with altered last layer\n",
        "embedding_model = deepcopy(model)  # start with a clone of the original model\n",
        "embedding_model.fc = torch.nn.Identity() # instead of classifier head - do nothing\n",
        "embedding_model = embedding_model.to(device)  # move to CUDA if available"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "dQRLvHltI1hI"
      },
      "outputs": [],
      "source": [
        "img = Image.open(requests.get('https://github.com/yandexdataschool/Practical_DL/blob/fall24/week04_finetuning/sample_images/albatross.jpg?raw=true', stream=True).raw)\n",
        "# img = Image.open('sample_images/albatross.jpg') # alt link\n",
        "img_torch = transforms.ToTensor()(img).unsqueeze(0).to(device)\n",
        "out = embedding_model(img_torch).cpu().data.numpy()\n",
        "assert out.shape == (1, 512), \"your output for single image should have shape (1, 512)\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "I2j4-Pq8I1hI"
      },
      "source": [
        "# Starter problem: cat-dog classification\n",
        "Your next task is to use a pre-trained model to distinguish between cats and dogs.\n",
        "- viewed as imposible in 2000\n",
        "- popular data science challenge problem in 2010\n",
        "- warm-up task for students in 2020s <br>\n",
        "![cat_meme](https://i.imgur.com/u1bubWv.jpeg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "Y_YauwSRI1hI"
      },
      "outputs": [],
      "source": [
        "# download the dataset\n",
        "!wget https://storage.yandexcloud.net/yandex-research/courses/dogs_vs_cats_1000.zip -O dogs_vs_cats_1000.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "hII7RmglI1hI"
      },
      "outputs": [],
      "source": [
        "!unzip -qn dogs_vs_cats_1000.zip\n",
        "!ls dogs_vs_cats_1000 | wc -l  # should be 2000 images extracted"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "FNbvtVBDI1hI"
      },
      "outputs": [],
      "source": [
        "# Sample pets images\n",
        "import random\n",
        "fig, axs = plt.subplots(1, 8, figsize=(16, 2))\n",
        "\n",
        "fnames = [fn for fn in os.listdir('dogs_vs_cats_1000')]\n",
        "for ax, fname in zip(axs.ravel(), random.choices(fnames, k=8)):\n",
        "    img_ = Image.open(os.path.join('dogs_vs_cats_1000', fname))\n",
        "    ax.imshow(img_)\n",
        "    ax.set_title(f\"{fname}\")\n",
        "    ax.tick_params(left = False,labelleft = False , labelbottom = False, bottom = False)\n",
        "plt.tight_layout()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 388,
          "referenced_widgets": [
            "ee986071846a4621b12a2b77d0e0db4a",
            "da7a942ee6a842a6b2fdfb99be6b7df1",
            "b0d18d4c942548f7874e14c9f0e6c7e0",
            "ed53c0ad7c824702b76ec1e161f51e01",
            "57915b9a2bf548d6907d350e78a50953",
            "52193e2e02e9465288a1802115be0db1",
            "03d6174f4b7e4513b72adf0b04d40efe",
            "47f9d5f4849548cc81e0ffe4280700e8",
            "087180a16067440a8db9babdf605221e",
            "32afdbbb2c3d434aa1d56a2fa46072f4",
            "1f61c81a9d274852b7edb822e56f7efa"
          ]
        },
        "id": "Wn47KntbI1hI",
        "outputId": "5e68f598-940f-45e2-c2df-2845f3df2754"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/2000 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ee986071846a4621b12a2b77d0e0db4a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1662082343.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0mx_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_batch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0;31m# use your embedding model to produce embeddings vectors, save results on CPU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m             \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membedding_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36m_forward_impl\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    274\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mavgpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    242\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    245\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    546\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 548\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    549\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    541\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    542\u001b[0m             )\n\u001b[0;32m--> 543\u001b[0;31m         return F.conv2d(\n\u001b[0m\u001b[1;32m    544\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdilation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    545\u001b[0m         )\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Here we generate image embeddings using the activations before the last layer\n",
        "# use batches to accelerate the process\n",
        "\n",
        "X_ = []  # storage for batches embeddings\n",
        "Y_ = []  # storage for batches labels\n",
        "\n",
        "filenames = [fname for fname in os.listdir('dogs_vs_cats_1000')]\n",
        "batch_size = 64\n",
        "x_batch_list = []  # to accumulate batch components\n",
        "\n",
        "with torch.no_grad():\n",
        "    for i, fname in enumerate(tqdm(filenames)):\n",
        "        img = Image.open(os.path.join(\"dogs_vs_cats_1000\", fname))\n",
        "        img_torch = transforms.ToTensor()(img.resize((224, 224)))\n",
        "        x_batch_list.append(img_torch)\n",
        "        Y_.append(1 if fname.startswith(\"cat\") else 0)\n",
        "\n",
        "        if len(x_batch_list) == batch_size or i >= len(filenames) - 1:\n",
        "            x_batch = torch.stack(x_batch_list)\n",
        "            # use your embedding model to produce embeddings vectors, save results on CPU\n",
        "            embeddings = embedding_model(x_batch.to(device)).cpu()\n",
        "\n",
        "            assert isinstance(embeddings,  torch.Tensor) and embeddings.device.type == \"cpu\"\n",
        "            assert embeddings.ndim == 2 and embeddings.shape[1] == 512\n",
        "            X_.append(embeddings)\n",
        "            x_batch_list = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "NimgcR4HI1hI"
      },
      "outputs": [],
      "source": [
        "X = np.concatenate(X_, axis = 0)  # concatenate all batches' embeddings into single 2D array.\n",
        "Y = np.array(Y_[:len(X)])  # convert labels into np array; crop if we ended prematurely\n",
        "\n",
        "print(X.shape, Y.shape, np.mean(Y))\n",
        "\n",
        "assert X.ndim == 2 and X.shape[1] == 512\n",
        "assert X.shape[0] == len(filenames)\n",
        "assert Y.ndim == 1 and Y.shape[0] == X.shape[0]\n",
        "assert 0.49 <= np.mean(Y) <= 0.51"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IkOamS-PI1hJ"
      },
      "source": [
        "### Using model embeddings as classifier features\n",
        "\n",
        "Train sklearn model, evaluate validation accuracy (should be >90%)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XGl3lLWFI1hJ"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
        "from sklearn.linear_model import LogisticRegression, RidgeClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "aAxmLdYnI1hJ"
      },
      "source": [
        "__Task 1 (2 points): train the model and evaluate its accuracy on a hold-out set.__\n",
        "- write the code for cats/dogs classification using the embeddings created above. Use any classification tools that you learned. Reach at least 95% accuracy. __(1 point)__.  Try few different tools if accuracy is not high enough. <br>\n",
        "(You may choose any classifier algorithm as long as it gets above 95% accuracy. To get the max grade here, you need to tune main hyperparameters of your chosen algorithm (e.g. k for KNN, tree depth, logreg C) depending on how many data points you have. )\n",
        "- try this excercise with much smaller training set. Find the lowest train set size at which you can still predict cat/dog class with 95% accuracy __(1 point)__ <br>\n",
        "(note: exact threshold may depend on algorightm choice and train/test split. Show your effort in  experimenting with low train set sizes.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "tqk4mrMxI1hJ"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "ts = 0.9\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=ts, random_state=12345)\n",
        "\n",
        "models = [\n",
        "    # ('Decision Tree', DecisionTreeClassifier(random_state=12345), {\n",
        "    #     'max_depth': [None, 5, 10, 20],\n",
        "    #     'min_samples_split': [2, 5, 10]\n",
        "    # }),\n",
        "    ('Logistic Regression', LogisticRegression(random_state=12345, solver='liblinear'), {\n",
        "        'C': [0.001, 0.01, 0.1, 1, 10, 100]\n",
        "    }),\n",
        "    ('Random Forest', RandomForestClassifier(random_state=12345), {\n",
        "        'n_estimators': [50, 100, 200],\n",
        "        'max_depth': [None, 10, 20]\n",
        "    }),\n",
        "    ('Extra Trees', ExtraTreesClassifier(random_state=12345), {\n",
        "        'n_estimators': [50, 100, 200],\n",
        "        'max_depth': [None, 10, 20]\n",
        "    }),\n",
        "    # ('Gradient Boosting', GradientBoostingClassifier(random_state=12345), {\n",
        "    #     'n_estimators': [50],\n",
        "    #     'learning_rate': [0.01, 0.1]\n",
        "    # }),\n",
        "    # ('AdaBoost', AdaBoostClassifier(random_state=12345), {\n",
        "    #     'n_estimators': [50],\n",
        "    #     'learning_rate': [0.01, 0.1]\n",
        "    # }),\n",
        "    ('Ridge Classifier', RidgeClassifier(random_state=12345), {\n",
        "        'alpha': [0.1, 1.0, 10.0]\n",
        "    }),\n",
        "    ('SVC', SVC(random_state=12345, probability=True), {\n",
        "        'C': [0.1, 1, 10],\n",
        "        'kernel': ['linear', 'rbf']\n",
        "    }),\n",
        "    ('KNeighbors Classifier', KNeighborsClassifier(), {\n",
        "        'n_neighbors': [3, 5, 7, 9]\n",
        "    })\n",
        "]\n",
        "\n",
        "best_score = 0\n",
        "best_model = None\n",
        "best_params = None\n",
        "\n",
        "for name, class_model, params in models:\n",
        "    print(f\"Running GridSearchCV for {name}...\")\n",
        "    grid_search = GridSearchCV(class_model, params, cv=5, scoring='accuracy', n_jobs=-1)\n",
        "    grid_search.fit(X_train, Y_train)\n",
        "\n",
        "    if grid_search.best_score_ > best_score:\n",
        "        best_score = grid_search.best_score_\n",
        "        best_model = grid_search.best_estimator_\n",
        "        best_params = grid_search.best_params_\n",
        "\n",
        "    print(f\"Best cross-validation accuracy for {name}: {grid_search.best_score_:.1%}\")\n",
        "    print(\"-\" * 30)\n",
        "\n",
        "print(\"\\nOverall Best Model:\")\n",
        "print(f\"Model: {type(best_model).__name__}\")\n",
        "print(f\"Best cross-validation accuracy: {best_score:.1%}\")\n",
        "\n",
        "# Evaluate the best model on the test set\n",
        "Y_pred = best_model.predict(X_test)\n",
        "test_accuracy = accuracy_score(Y_test, Y_pred)\n",
        "print(f\"Test set accuracy of the best model: {test_accuracy:.1%} with test size {ts}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "h8zFFrTKI1hJ"
      },
      "source": [
        "## Torchvision Datasets\n",
        "- Built-in datasets https://pytorch.org/vision/stable/datasets.html#built-in-datasets\n",
        "\n",
        "- Datasets and Dataloaders at `torch.utils.data`: https://pytorch.org/tutorials/beginner/basics/data_tutorial.html\n",
        "\n",
        "- Torchvision classes for custom datasets  https://pytorch.org/vision/stable/datasets.html#base-classes-for-custom-datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x8j075wzI1hJ"
      },
      "source": [
        "# Problem 2: Clasification of cat/dog breeds using Oxford pets dataset\n",
        "\n",
        "The next problem is to classify specific cat / dog breeds from popular Oxford pets dataset. It is conveniently provided by Pytorch so loading and using it is very easy.\n",
        "\n",
        "Dataset home page: https://www.robots.ox.ac.uk/~vgg/data/pets/\n",
        "\n",
        "Available from Pytorch: https://pytorch.org/vision/stable/generated/torchvision.datasets.OxfordIIITPet.html#torchvision.datasets.OxfordIIITPet\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "HHnOUfoWI1hJ"
      },
      "outputs": [],
      "source": [
        "# Loading train and test subsets of the dataset\n",
        "# using simple transform for both slices\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Resize((224, 224), antialias=True),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "train_dataset = torchvision.datasets.OxfordIIITPet(root='.', split='trainval', target_types='category', download=True, transform=test_transform)\n",
        "test_dataset = torchvision.datasets.OxfordIIITPet(root='.', split='test', target_types='category', download=True, transform=test_transform)\n",
        "\n",
        "print(\"dataset sizes:\", len(train_dataset), len(test_dataset))\n",
        "\n",
        "print(train_dataset.classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "u3dWoSjYI1hJ"
      },
      "outputs": [],
      "source": [
        "# create dataloaders to repack the datasets' data into batches\n",
        "# read more\n",
        "\n",
        "batch_size = 64\n",
        "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "print(\"dataloader sizes:\", len(train_dataloader), len(test_dataloader))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "0sMMiyMoI1hK"
      },
      "outputs": [],
      "source": [
        "# Showing sample pets images\n",
        "for x_batch, y_batch in train_dataloader:\n",
        "    break  # Only get data from the 1st batch for now\n",
        "\n",
        "fig, axs = plt.subplots(2, 8, figsize=(16, 4))\n",
        "for i, ax in enumerate(axs.ravel()):\n",
        "    img_ = x_batch[i].permute(1, 2, 0) # restoring dimensions order\n",
        "    img_ -= img_.min().item()  # normalizing image to 0..1\n",
        "    img_ /= img_.max().item()\n",
        "    ax.imshow(img_)\n",
        "    label = train_dataset.classes[y_batch[i]]\n",
        "    ax.set_title(f\"{label}\")\n",
        "    ax.tick_params(left=False, labelleft=False, labelbottom=False, bottom=False)\n",
        "plt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9hCZ1FEI1hT"
      },
      "source": [
        "Remember that Imagenet includes quite a few cat and dog breeds among its classes.\n",
        "\n",
        "Let's try to predcit breeds with Imagenet-pretrained model without finetuning first"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "japb6QT7I1hT"
      },
      "outputs": [],
      "source": [
        "# Predicting breeds with Imagenet-pretrained model\n",
        "Y_probs, Y_true = [], []\n",
        "for x_batch, y_batch in tqdm(test_dataloader):\n",
        "    with torch.no_grad():\n",
        "        prediction = model(x_batch.to(device))\n",
        "        probs = torch.nn.functional.softmax(prediction, dim=-1).cpu()\n",
        "        Y_true.append(y_batch)\n",
        "        Y_probs.append(probs)\n",
        "\n",
        "Y_probs = torch.cat(Y_probs, axis=0)  # tensor with class probabilities\n",
        "Y_true = torch.cat(Y_true)\n",
        "Y_true.shape, Y_probs.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "HOtkstGlI1hT"
      },
      "outputs": [],
      "source": [
        "# Output top 3 predictions for each class using pretrained CNN (no finetuning)\n",
        "# Expect to see high accuracy (70-80%) in some classes, much lower overal, mismatch in label spaces.\n",
        "\n",
        "results_list = {}\n",
        "\n",
        "for i, cl in enumerate(test_dataset.classes):\n",
        "    class_stats = {\"true_label\":cl}\n",
        "    probs1 = Y_probs[np.where(Y_true == i)].mean(axis=0).numpy()\n",
        "    top_ix = probs1.argsort()[-1:][::-1]\n",
        "    class_stats\n",
        "    for j, l in enumerate(top_ix):\n",
        "        class_stats = {f\"pred_1\": imagenet_labels[l],\n",
        "                    f\"prob_{1}\": probs1.ravel()[l]}\n",
        "    results_list[cl] = class_stats\n",
        "\n",
        "df = pd.DataFrame(results_list).T.sort_values('prob_1', ascending=False)\n",
        "float_cols = ['prob_1']\n",
        "df.style.format('{:.2%}', subset=float_cols)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oHP5G0KpI1hT"
      },
      "source": [
        "### Finetuning CNN: classification layer only:\n",
        "\n",
        "__Task 2: (3 points)__:<br>\n",
        "Complete the code below and reach at least 85% accuracy by training only the classification layer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "tKm7BxPrI1hT"
      },
      "outputs": [],
      "source": [
        "ft_model = deepcopy(model).to(device)\n",
        "\n",
        "# 1) freeze all model parameters\n",
        "for param in ft_model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# 2) replace model.fc with a new linear layer that has the appropriate number of outputs\n",
        "num_ftrs = ft_model.fc.in_features\n",
        "ft_model.fc = torch.nn.Linear(num_ftrs, len(train_dataset.classes)).to(device)\n",
        "\n",
        "\n",
        "assert all(not p.requires_grad for p in ft_model.parameters() if p not in set(ft_model.fc.parameters()))\n",
        "assert ft_model.fc.out_features == len(train_dataset.classes)\n",
        "assert ft_model.fc.weight.device == device, f\"ft_model.fc must be on {device}\"\n",
        "assert ft_model.fc.weight.requires_grad\n",
        "\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(ft_model.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "Q-Xb8zAxI1hU"
      },
      "outputs": [],
      "source": [
        "# train the finetuning model in a standard training loop.\n",
        "\n",
        "num_epochs = 10  # Adjust number of epochs if needed\n",
        "\n",
        "history = defaultdict(list)\n",
        "for epoch in range(num_epochs):\n",
        "    start_time = time.time()\n",
        "\n",
        "    ft_model.train()\n",
        "    train_losses = []\n",
        "    pbar = tqdm(train_dataloader, leave=False)\n",
        "    for x_batch, y_batch in pbar:\n",
        "\n",
        "        # calculate model predictions, calculate loss, make optimizer step\n",
        "        loss = criterion(ft_model(x_batch.to(device)), y_batch.to(device))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        train_losses.append(loss.item())\n",
        "        pbar.desc = f\"Train. Ep:{epoch}, Loss:{np.mean(train_losses[-10:]):.5f}\"\n",
        "\n",
        "    ft_model.eval()\n",
        "    val_losses = []\n",
        "    val_cnt, val_correct = 0, 0\n",
        "    pbar = tqdm(test_dataloader, leave=False)\n",
        "    for x_batch, y_batch in pbar:\n",
        "\n",
        "        # get model predictions, measure loss, collect data for accuracy calc\n",
        "        logits = ft_model(torch.as_tensor(x_batch, dtype=torch.float32, device=device))\n",
        "        y_pred = logits.argmax(-1).detach().to(\"cpu\").numpy()\n",
        "        loss = criterion(logits, torch.as_tensor(y_batch, dtype=torch.long, device=device))\n",
        "\n",
        "        val_losses.append(loss.item())\n",
        "        val_cnt += y_batch.shape[0]\n",
        "        val_correct += (y_batch.cuda() == torch.argmax(y_pred, dim=1)).sum().item()\n",
        "        pbar.desc = f\"Valid. Ep:{epoch}, Loss:{np.mean(val_losses):.5f}\"\n",
        "\n",
        "    train_loss = np.mean(train_losses)\n",
        "    val_loss = np.mean(val_losses)\n",
        "    accuracy = val_correct / val_cnt\n",
        "    history['train_loss'].append(train_loss)\n",
        "    history['val_loss'].append(val_loss)\n",
        "    history['accuracy'].append(accuracy)\n",
        "    print(f\"Ep.{epoch:>2}: {train_loss=:.5f}  {val_loss=:.5f}  {accuracy=:.2%}  epoch_time={time.time() - start_time:.1f}s\")\n",
        "\n",
        "    if epoch > 2 and history['accuracy'][-1] < max(history['accuracy']):\n",
        "        break\n",
        "print(f\"Best Accuracy = {max(history['accuracy']):.2%}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U3LkAiW0I1hU"
      },
      "outputs": [],
      "source": [
        "# This part of finetuning alone should deliver at least 85% accuracy.\n",
        "assert accuracy > 0.85"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wWKJGQjvI1hU"
      },
      "outputs": [],
      "source": [
        "# Predicting breeds with finetuned model\n",
        "\n",
        "Y_probs, Y_true = [], []\n",
        "for x_batch, y_batch in tqdm(test_dataloader):\n",
        "    with torch.no_grad():\n",
        "        prediction = ft_model(x_batch.to(device))\n",
        "        probs = torch.nn.functional.softmax(prediction, dim=-1).cpu()\n",
        "        Y_true.append(y_batch)\n",
        "        Y_probs.append(probs)\n",
        "\n",
        "Y_probs = torch.cat(Y_probs, axis=0)\n",
        "Y_true = torch.cat(Y_true)\n",
        "Y_true.shape, Y_probs.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "ZmFKs2ZEI1hU"
      },
      "outputs": [],
      "source": [
        "# Measure Top1 prediction accuracy for each class\n",
        "\n",
        "results_list = {}\n",
        "total, correct = 0, 0\n",
        "\n",
        "for i, cl in enumerate(test_dataset.classes):\n",
        "    class_preds = np.where(Y_true == i)\n",
        "    probs1 = Y_probs[class_preds].mean(axis=0).numpy()\n",
        "    acc1 = probs1[i]\n",
        "    class_stats = {\"true_label\":cl, 'acc1': acc1}\n",
        "    total += class_preds[0].shape[0]\n",
        "    correct += class_preds[0].shape[0] * acc1\n",
        "    results_list[cl] = acc1\n",
        "\n",
        "df = pd.Series(results_list).to_frame('acc1').sort_values('acc1', ascending=False)\n",
        "df.style.format('{:.2%}', subset=['acc1'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KkzoS2YvI1hU"
      },
      "source": [
        "### Finetuning CNN: now unfreeze and train all layers:\n",
        "\n",
        "__task 3 (5 points)__: Based on the previous task, continue finetuning the model run by training all of its layers. Obtain at least 90% accuracy (macro average, as defined in the code).\n",
        "To reach 90%, you will need to experiment with network and/or training code, start from using improvements from the list below:\n",
        "\n",
        "Scoring:\n",
        "- reaching 90% accuracy - __3 points__\n",
        "- testing out all 4 of these improvements - __2 points__:\n",
        "    - Hyperparameters optimization\n",
        "    - Image augmentation\n",
        "    - Pretrained model selection\n",
        "    - Early stopping training at epoch with best metric and / or loading the weights of best epoch. [read about `state_dict`](https://pytorch.org/tutorials/beginner/saving_loading_models.html#what-is-a-state-dict)\n",
        "    \n",
        "(you may not necessarily get spectacular results right away. Experiment and describe your actions and findings in a brief report in this notebook.)\n",
        "- scoring above 90% -- __+1 BONUS point__ for every 1% improvement in accuracy above 90%\n",
        "\n",
        "__Guidelines__: to improve quality of the finetuned model we can now train all of its layers.\n",
        "- do this after initial training of classification layer\n",
        "- set learning rate to much lower level to avoid explosion\n",
        "- make all layers trainable using `requires_grad` param"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "t1gV42QMI1hU"
      },
      "outputs": [],
      "source": [
        "# Continue training with all layers involved\n",
        "\n",
        "# <YOUR CODE to make all layers trainable>\n",
        "\n",
        "assert all (p.requires_grad for p in ft_model.parameters())\n",
        "\n",
        "optimizer = torch.optim.Adam(ft_model.parameters(), lr=0.00003)  # reduce LR to avoid explosion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "Z681Ush4I1hU"
      },
      "outputs": [],
      "source": [
        "<TRAIN MORE - COPY TRAINING CODE FROM ABOVE>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "0D8hsxkLI1hV"
      },
      "outputs": [],
      "source": [
        "<MEASURE RESULTS - COPY ACCURACY MEASUREMENT CODE FROM ABOVE>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-KzTtLZ_I1hV"
      },
      "outputs": [],
      "source": [
        "print(f\"Overall Accuracy = {correct / total:.2%}\")  # macro-averaged accuracy\n",
        "assert correct / total >= 0.9"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0kDT-8XoI1hV"
      },
      "outputs": [],
      "source": [
        "<RUN EXPERIMENTS WITH IMPROVEMENTS>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L7082CDVI1hV"
      },
      "source": [
        "# STUDENT'S REPORT\n",
        "\n",
        "Please write a short report describing the steps you took to improve quality. It should be at least a short paragraph with 3-5 sentences and, optionally, any relevant numbers/charts, but there's no upper limit.\n",
        "\n",
        "Here's what i did: `<YOUR TEXT>`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "tags": [],
        "id": "bVduahozI1hV"
      },
      "source": [
        "# Bonus task __(optional. 2 more points)__:\n",
        "\n",
        "Notice that accuracy varies across breeds. Build confusion Propose improvements to the model finetuning process to improve minimal accuracy levels for all classes. Show the model runs.\n",
        "Measure the improvement in terms of minimal accuracy and micro-averaged accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RCiCJSKkI1hV"
      },
      "source": [
        "# EXTRA STUFF:\n",
        "\n",
        "\n",
        "## Links:    \n",
        "- [The Building Blocks of Interpretability](https://distill.pub/2018/building-blocks/)\n",
        "- [Activations Atlas](https://distill.pub/2019/activation-atlas/)\n",
        "- more models from https://github.com/huggingface/pytorch-image-models\n",
        "- Papers with code: benchmarks and model references. https://paperswithcode.com/sota/image-classification-on-imagenet"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": []
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "ee986071846a4621b12a2b77d0e0db4a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_da7a942ee6a842a6b2fdfb99be6b7df1",
              "IPY_MODEL_b0d18d4c942548f7874e14c9f0e6c7e0",
              "IPY_MODEL_ed53c0ad7c824702b76ec1e161f51e01"
            ],
            "layout": "IPY_MODEL_57915b9a2bf548d6907d350e78a50953"
          }
        },
        "da7a942ee6a842a6b2fdfb99be6b7df1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_52193e2e02e9465288a1802115be0db1",
            "placeholder": "​",
            "style": "IPY_MODEL_03d6174f4b7e4513b72adf0b04d40efe",
            "value": "  6%"
          }
        },
        "b0d18d4c942548f7874e14c9f0e6c7e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_47f9d5f4849548cc81e0ffe4280700e8",
            "max": 2000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_087180a16067440a8db9babdf605221e",
            "value": 127
          }
        },
        "ed53c0ad7c824702b76ec1e161f51e01": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_32afdbbb2c3d434aa1d56a2fa46072f4",
            "placeholder": "​",
            "style": "IPY_MODEL_1f61c81a9d274852b7edb822e56f7efa",
            "value": " 127/2000 [00:13&lt;02:24, 12.99it/s]"
          }
        },
        "57915b9a2bf548d6907d350e78a50953": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "52193e2e02e9465288a1802115be0db1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "03d6174f4b7e4513b72adf0b04d40efe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "47f9d5f4849548cc81e0ffe4280700e8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "087180a16067440a8db9babdf605221e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "32afdbbb2c3d434aa1d56a2fa46072f4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1f61c81a9d274852b7edb822e56f7efa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}