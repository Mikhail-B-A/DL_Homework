{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Bystrov Mikhail. Homework 3"
      ],
      "metadata": {
        "id": "g5flwJdYxwQd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/yandexdataschool/Practical_DL/refs/heads/fall25/week03_convnets/cifar.py"
      ],
      "metadata": {
        "id": "qcifiebYVzBd",
        "outputId": "5c96be0d-122b-4edf-cfe8-8157203215a7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-09-26 07:52:35--  https://raw.githubusercontent.com/yandexdataschool/Practical_DL/refs/heads/fall25/week03_convnets/cifar.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2396 (2.3K) [text/plain]\n",
            "Saving to: ‘cifar.py.1’\n",
            "\n",
            "\rcifar.py.1            0%[                    ]       0  --.-KB/s               \rcifar.py.1          100%[===================>]   2.34K  --.-KB/s    in 0s      \n",
            "\n",
            "2025-09-26 07:52:35 (48.4 MB/s) - ‘cifar.py.1’ saved [2396/2396]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from cifar import load_cifar10\n",
        "X_train, y_train, X_val, y_val, X_test, y_test = load_cifar10(\"cifar_data\")\n",
        "\n",
        "class_names = np.array(['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
        "                        'dog', 'frog', 'horse', 'ship', 'truck'])"
      ],
      "metadata": {
        "id": "oCRfewSAV0FG"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import time"
      ],
      "metadata": {
        "id": "Ml53mJ8HV2Hf"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = nn.Sequential(\n",
        "    nn.Conv2d(3, 32, kernel_size=(3,3), padding=1),\n",
        "    nn.BatchNorm2d(32),\n",
        "    nn.ReLU(),\n",
        "    nn.Conv2d(32, 64, kernel_size=(3,3), padding=1),\n",
        "    nn.BatchNorm2d(64),\n",
        "    nn.MaxPool2d((2, 2)),\n",
        "    nn.LeakyReLU(0.1),\n",
        "    nn.Conv2d(64, 128, kernel_size=(3,3), padding=1),\n",
        "    nn.BatchNorm2d(128),\n",
        "    nn.ReLU(),\n",
        "    nn.Conv2d(128, 256, kernel_size=(3,3), padding=1),\n",
        "    nn.BatchNorm2d(256),\n",
        "    nn.MaxPool2d((2, 2)),\n",
        "    nn.LeakyReLU(0.1),\n",
        "    nn.Conv2d(256, 512, kernel_size=(3,3), padding=1),\n",
        "    nn.BatchNorm2d(512),\n",
        "    nn.MaxPool2d((2, 2)),\n",
        "    nn.ReLU(),\n",
        "    nn.Flatten(),\n",
        "    nn.Linear(512 * 4 * 4, 256),\n",
        "    nn.LeakyReLU(0.1),\n",
        "    nn.Dropout(0.25),\n",
        "    nn.Linear(256, 128),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(0.25),\n",
        "    nn.Linear(128, 10)\n",
        ").to(device)"
      ],
      "metadata": {
        "id": "ZgVyuVIQLWDi"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "opt = torch.optim.Adam(model.parameters())\n",
        "\n",
        "train_loss = []\n",
        "val_accuracy = []\n",
        "\n",
        "# An auxilary function that returns mini-batches for neural network training\n",
        "def iterate_minibatches(X, y, batchsize):\n",
        "    indices = np.random.permutation(np.arange(len(X)))\n",
        "    for start in range(0, len(indices), batchsize):\n",
        "        ix = indices[start: start + batchsize]\n",
        "        yield X[ix], y[ix]"
      ],
      "metadata": {
        "id": "HI0UgvNI03PA"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_loss(X_batch, y_batch):\n",
        "    X_batch = torch.as_tensor(X_batch, dtype=torch.float32, device=device)\n",
        "    y_batch = torch.as_tensor(y_batch, dtype=torch.int64, device=device)\n",
        "    logits = model(X_batch)\n",
        "    return F.cross_entropy(logits, y_batch).mean()"
      ],
      "metadata": {
        "id": "1NO_Z2ZC1dXk"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "opt = torch.optim.Adam(model.parameters())\n",
        "\n",
        "train_loss = []\n",
        "val_accuracy = []\n",
        "\n",
        "num_epochs = 100 # total amount of full passes over training data\n",
        "batch_size = 50  # number of samples processed in one SGD iteration\n",
        "\n",
        "best_accuracy = -1.0\n",
        "best_epoch = -1\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # In each epoch, we do a full pass over the training data:\n",
        "    start_time = time.time()\n",
        "    model.train(True) # enable dropout / batch_norm training behavior\n",
        "    for X_batch, y_batch in iterate_minibatches(X_train, y_train, batch_size):\n",
        "        # train on batch\n",
        "        loss = compute_loss(X_batch, y_batch)\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "        opt.zero_grad()\n",
        "        train_loss.append(loss.item())  # .item() = convert 1-value Tensor to float\n",
        "\n",
        "    # And a full pass over the validation data:\n",
        "    model.train(False)     # disable dropout / use averages for batch_norm\n",
        "    with torch.no_grad():  # do not store intermediate activations\n",
        "        epoch_val_accuracy = [] # Calculate accuracy for the current epoch's validation pass\n",
        "        for X_batch, y_batch in iterate_minibatches(X_val, y_val, batch_size):\n",
        "            logits = model(torch.as_tensor(X_batch, dtype=torch.float32, device=device))\n",
        "            y_pred = logits.argmax(-1).detach().to(\"cpu\").numpy()\n",
        "            epoch_val_accuracy.append(np.mean(y_batch == y_pred))\n",
        "\n",
        "        current_accuracy = np.mean(epoch_val_accuracy) # Mean accuracy for the current epoch\n",
        "        val_accuracy.append(current_accuracy) # Append epoch accuracy to the list\n",
        "\n",
        "\n",
        "    # Then we print the results for this epoch:\n",
        "    print(\"Epoch {} of {} took {:.3f}s\".format(\n",
        "        epoch + 1, num_epochs, time.time() - start_time))\n",
        "    print(\"  training loss (in-iteration): \\t{:.6f}\".format(\n",
        "        np.mean(train_loss[-len(X_train) // batch_size :])))\n",
        "    print(\"  validation accuracy: \\t\\t\\t{:.2f} %\".format(\n",
        "        current_accuracy * 100)) # Use current_accuracy here\n",
        "\n",
        "    # Early stopping\n",
        "    if current_accuracy > best_accuracy:\n",
        "      best_accuracy = current_accuracy\n",
        "      best_epoch = epoch\n",
        "      torch.save(model.state_dict(), \"best_state.pt\")\n",
        "    elif epoch - best_epoch > 10:\n",
        "      print(f\"  Validation accuracy has not improved for 10 epochs. Stopping early at epoch {epoch + 1}.\")\n",
        "      break\n",
        "\n",
        "\n",
        "# Load the best model state\n",
        "if os.path.exists(\"best_state.pt\"):\n",
        "    model.load_state_dict(torch.load(\"best_state.pt\"))\n",
        "    print(\"Loaded best model state.\")\n",
        "\n",
        "\n",
        "model.train(False) # disable dropout / use averages for batch_norm\n",
        "test_batch_acc = []\n",
        "with torch.no_grad():\n",
        "    for X_batch, y_batch in iterate_minibatches(X_test, y_test, 500):\n",
        "        logits = model(torch.as_tensor(X_batch, dtype=torch.float32, device=device))\n",
        "        y_pred = logits.max(1)[1].detach().cpu().numpy()\n",
        "        test_batch_acc.append(np.mean(y_batch == y_pred))\n",
        "\n",
        "test_accuracy = np.mean(test_batch_acc)\n",
        "\n",
        "print(\"Final results:\")\n",
        "print(\"  test accuracy:\\t\\t{:.2f} %\".format(\n",
        "    test_accuracy * 100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QmTW7BvTz9Rx",
        "outputId": "a27fcd99-c155-42c2-f485-e4123e3193ea"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 of 100 took 12.929s\n",
            "  training loss (in-iteration): \t1.553493\n",
            "  validation accuracy: \t\t\t56.77 %\n",
            "Epoch 2 of 100 took 11.715s\n",
            "  training loss (in-iteration): \t1.074368\n",
            "  validation accuracy: \t\t\t65.10 %\n",
            "Epoch 3 of 100 took 11.824s\n",
            "  training loss (in-iteration): \t0.860482\n",
            "  validation accuracy: \t\t\t72.89 %\n",
            "Epoch 4 of 100 took 11.978s\n",
            "  training loss (in-iteration): \t0.716502\n",
            "  validation accuracy: \t\t\t77.04 %\n",
            "Epoch 5 of 100 took 12.088s\n",
            "  training loss (in-iteration): \t0.613991\n",
            "  validation accuracy: \t\t\t76.33 %\n",
            "Epoch 6 of 100 took 12.239s\n",
            "  training loss (in-iteration): \t0.516839\n",
            "  validation accuracy: \t\t\t76.60 %\n",
            "Epoch 7 of 100 took 12.381s\n",
            "  training loss (in-iteration): \t0.427204\n",
            "  validation accuracy: \t\t\t79.13 %\n",
            "Epoch 8 of 100 took 12.868s\n",
            "  training loss (in-iteration): \t0.349260\n",
            "  validation accuracy: \t\t\t82.75 %\n",
            "Epoch 9 of 100 took 12.629s\n",
            "  training loss (in-iteration): \t0.288750\n",
            "  validation accuracy: \t\t\t82.53 %\n",
            "Epoch 10 of 100 took 12.239s\n",
            "  training loss (in-iteration): \t0.223451\n",
            "  validation accuracy: \t\t\t82.99 %\n",
            "Epoch 11 of 100 took 12.178s\n",
            "  training loss (in-iteration): \t0.180897\n",
            "  validation accuracy: \t\t\t83.81 %\n",
            "Epoch 12 of 100 took 12.316s\n",
            "  training loss (in-iteration): \t0.143345\n",
            "  validation accuracy: \t\t\t83.94 %\n",
            "Epoch 13 of 100 took 12.213s\n",
            "  training loss (in-iteration): \t0.118630\n",
            "  validation accuracy: \t\t\t82.84 %\n",
            "Epoch 14 of 100 took 12.260s\n",
            "  training loss (in-iteration): \t0.102941\n",
            "  validation accuracy: \t\t\t83.96 %\n",
            "Epoch 15 of 100 took 12.282s\n",
            "  training loss (in-iteration): \t0.089066\n",
            "  validation accuracy: \t\t\t83.97 %\n",
            "Epoch 16 of 100 took 12.261s\n",
            "  training loss (in-iteration): \t0.077003\n",
            "  validation accuracy: \t\t\t84.10 %\n",
            "Epoch 17 of 100 took 12.259s\n",
            "  training loss (in-iteration): \t0.069308\n",
            "  validation accuracy: \t\t\t82.57 %\n",
            "Epoch 18 of 100 took 12.242s\n",
            "  training loss (in-iteration): \t0.067830\n",
            "  validation accuracy: \t\t\t84.10 %\n",
            "Epoch 19 of 100 took 12.248s\n",
            "  training loss (in-iteration): \t0.060499\n",
            "  validation accuracy: \t\t\t84.58 %\n",
            "Epoch 20 of 100 took 12.242s\n",
            "  training loss (in-iteration): \t0.056413\n",
            "  validation accuracy: \t\t\t84.76 %\n",
            "Epoch 21 of 100 took 12.277s\n",
            "  training loss (in-iteration): \t0.057545\n",
            "  validation accuracy: \t\t\t83.38 %\n",
            "Epoch 22 of 100 took 12.242s\n",
            "  training loss (in-iteration): \t0.054151\n",
            "  validation accuracy: \t\t\t84.71 %\n",
            "Epoch 23 of 100 took 12.262s\n",
            "  training loss (in-iteration): \t0.048911\n",
            "  validation accuracy: \t\t\t84.29 %\n",
            "Epoch 24 of 100 took 12.291s\n",
            "  training loss (in-iteration): \t0.046439\n",
            "  validation accuracy: \t\t\t83.78 %\n",
            "Epoch 25 of 100 took 12.302s\n",
            "  training loss (in-iteration): \t0.047967\n",
            "  validation accuracy: \t\t\t84.43 %\n",
            "Epoch 26 of 100 took 12.293s\n",
            "  training loss (in-iteration): \t0.044559\n",
            "  validation accuracy: \t\t\t84.36 %\n",
            "Epoch 27 of 100 took 12.273s\n",
            "  training loss (in-iteration): \t0.035792\n",
            "  validation accuracy: \t\t\t83.45 %\n",
            "Epoch 28 of 100 took 12.271s\n",
            "  training loss (in-iteration): \t0.047376\n",
            "  validation accuracy: \t\t\t84.08 %\n",
            "Epoch 29 of 100 took 12.276s\n",
            "  training loss (in-iteration): \t0.036224\n",
            "  validation accuracy: \t\t\t83.70 %\n",
            "Epoch 30 of 100 took 12.272s\n",
            "  training loss (in-iteration): \t0.038160\n",
            "  validation accuracy: \t\t\t81.98 %\n",
            "Epoch 31 of 100 took 12.244s\n",
            "  training loss (in-iteration): \t0.037654\n",
            "  validation accuracy: \t\t\t83.85 %\n",
            "  Validation accuracy has not improved for 10 epochs. Stopping early at epoch 31.\n",
            "Loaded best model state.\n",
            "Final results:\n",
            "  test accuracy:\t\t83.44 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Why did it work?\n",
        "- First of all, the increase in the complexity of the neural network in 5 convolutional layers helped.\n",
        "- Using small $3 \\times 3$ convolutions with padding preserves spatial resolution while keeping the parameter count efficient.\n",
        "- Batch normalization and a mix of ReLU/LeakyReLU activations stabilize training and improve gradient flow.\n",
        "- MaxPooling layers reduce spatial dimensions, focusing the deeper layers on more abstract features.\n",
        "- Finally, dropout in the fully connected layers prevents overfitting and improves generalization."
      ],
      "metadata": {
        "id": "InIQB8rxCopb"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qlw_jvxuDjGn"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}