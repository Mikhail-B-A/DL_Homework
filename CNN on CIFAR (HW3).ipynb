{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Bystrov Mikhail. Homework 3"
      ],
      "metadata": {
        "id": "g5flwJdYxwQd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/yandexdataschool/Practical_DL/refs/heads/fall25/week03_convnets/cifar.py"
      ],
      "metadata": {
        "id": "qcifiebYVzBd",
        "outputId": "552c2a50-f3d5-4df1-f4c9-c839ea5e0e25",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 233,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-09-25 14:33:27--  https://raw.githubusercontent.com/yandexdataschool/Practical_DL/refs/heads/fall25/week03_convnets/cifar.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2396 (2.3K) [text/plain]\n",
            "Saving to: ‘cifar.py.15’\n",
            "\n",
            "cifar.py.15         100%[===================>]   2.34K  --.-KB/s    in 0s      \n",
            "\n",
            "2025-09-25 14:33:27 (48.2 MB/s) - ‘cifar.py.15’ saved [2396/2396]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from cifar import load_cifar10\n",
        "X_train, y_train, X_val, y_val, X_test, y_test = load_cifar10(\"cifar_data\")\n",
        "\n",
        "class_names = np.array(['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
        "                        'dog', 'frog', 'horse', 'ship', 'truck'])"
      ],
      "metadata": {
        "id": "oCRfewSAV0FG"
      },
      "execution_count": 234,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "Ml53mJ8HV2Hf"
      },
      "execution_count": 235,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gE_NtOG6Tpy9"
      },
      "source": [
        "## Task I: small convolution net\n",
        "### First step\n",
        "\n",
        "Let's create a mini-convolutional network with roughly such architecture:\n",
        "* Input layer\n",
        "* 3x3 convolution with 10 filters and _ReLU_ activation\n",
        "* 2x2 pooling (or set previous convolution stride to 3)\n",
        "* Flatten\n",
        "* Dense layer with 100 neurons and _ReLU_ activation\n",
        "* 10% dropout\n",
        "* Output dense layer.\n",
        "\n",
        "\n",
        "__Convolutional layers__ in torch are just like all other layers, but with a specific set of parameters:\n",
        "\n",
        "__`...`__\n",
        "\n",
        "__`model.add_module('conv1', nn.Conv2d(in_channels=3, out_channels=10, kernel_size=3)) # convolution`__\n",
        "\n",
        "__`model.add_module('pool1', nn.MaxPool2d(2)) # max pooling 2x2`__\n",
        "\n",
        "__`...`__\n",
        "\n",
        "\n",
        "Once you're done (and compute_loss no longer raises errors), train it with __Adam__ optimizer with default params (feel free to modify the code above).\n",
        "\n",
        "If everything is right, you should get at least __50%__ validation accuracy."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = nn.Sequential(\n",
        "    nn.Conv2d(3, 32, kernel_size=(3,3)),\n",
        "    nn.ReLU(),\n",
        "    nn.MaxPool2d((2, 2)),\n",
        "    nn.Flatten(),\n",
        "    nn.Linear(32 * 15 * 15, 100),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(0.1),\n",
        "    nn.Linear(in_features=100, out_features=10)\n",
        ").to(device)"
      ],
      "metadata": {
        "id": "H3zMdkaP2mR0"
      },
      "execution_count": 236,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model = nn.Sequential(\n",
        "#     nn.Conv2d(3, 32, kernel_size=(3,3)),\n",
        "#     nn.MaxPool2d((2, 2)),\n",
        "#     nn.ReLU(),\n",
        "#     nn.Conv2d(32, 64, kernel_size=(3,3)),\n",
        "#     nn.MaxPool2d((2, 2)),\n",
        "#     nn.ReLU(),\n",
        "#     nn.Flatten(),\n",
        "#     nn.Linear(64 * 6 * 6, 128),\n",
        "#     nn.ReLU(),\n",
        "#     nn.Linear(128, 10)\n",
        "# )"
      ],
      "metadata": {
        "id": "0RpLpWtoysTG"
      },
      "execution_count": 237,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "opt = torch.optim.Adam(model.parameters())\n",
        "\n",
        "train_loss = []\n",
        "val_accuracy = []\n",
        "\n",
        "# An auxilary function that returns mini-batches for neural network training\n",
        "def iterate_minibatches(X, y, batchsize):\n",
        "    indices = np.random.permutation(np.arange(len(X)))\n",
        "    for start in range(0, len(indices), batchsize):\n",
        "        ix = indices[start: start + batchsize]\n",
        "        yield X[ix], y[ix]"
      ],
      "metadata": {
        "id": "HI0UgvNI03PA"
      },
      "execution_count": 238,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_loss(X_batch, y_batch):\n",
        "    X_batch = torch.as_tensor(X_batch, dtype=torch.float32, device=device)\n",
        "    y_batch = torch.as_tensor(y_batch, dtype=torch.int64, device=device)\n",
        "    logits = model(X_batch)\n",
        "    return F.cross_entropy(logits, y_batch).mean()"
      ],
      "metadata": {
        "id": "1NO_Z2ZC1dXk"
      },
      "execution_count": 239,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "num_epochs = 0 # total amount of full passes over training data\n",
        "batch_size = 50  # number of samples processed in one SGD iteration\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # In each epoch, we do a full pass over the training data:\n",
        "    start_time = time.time()\n",
        "    model.train(True) # enable dropout / batch_norm training behavior\n",
        "    for X_batch, y_batch in iterate_minibatches(X_train, y_train, batch_size):\n",
        "        # train on batch\n",
        "        loss = compute_loss(X_batch, y_batch)\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "        opt.zero_grad()\n",
        "        train_loss.append(loss.item())  # .item() = convert 1-value Tensor to float\n",
        "\n",
        "    # And a full pass over the validation data:\n",
        "    model.train(False)     # disable dropout / use averages for batch_norm\n",
        "    with torch.no_grad():  # do not store intermediate activations\n",
        "        for X_batch, y_batch in iterate_minibatches(X_val, y_val, batch_size):\n",
        "            logits = model(torch.as_tensor(X_batch, dtype=torch.float32, device=device))\n",
        "            y_pred = logits.argmax(-1).detach().to(\"cpu\").numpy()\n",
        "            val_accuracy.append(np.mean(y_batch == y_pred))\n",
        "\n",
        "    # Then we print the results for this epoch:\n",
        "    print(\"Epoch {} of {} took {:.3f}s\".format(\n",
        "        epoch + 1, num_epochs, time.time() - start_time))\n",
        "    print(\"  training loss (in-iteration): \\t{:.6f}\".format(\n",
        "        np.mean(train_loss[-len(X_train) // batch_size :])))\n",
        "    print(\"  validation accuracy: \\t\\t\\t{:.2f} %\".format(\n",
        "        np.mean(val_accuracy[-len(X_val) // batch_size :]) * 100))"
      ],
      "metadata": {
        "id": "RTeC56kE078a"
      },
      "execution_count": 240,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.train(False) # disable dropout / use averages for batch_norm\n",
        "test_batch_acc = []\n",
        "for X_batch, y_batch in iterate_minibatches(X_test, y_test, 500):\n",
        "    logits = model(torch.as_tensor(X_batch, dtype=torch.float32, device=device))\n",
        "    y_pred = logits.max(1)[1].detach().to(\"cpu\").numpy()\n",
        "    test_batch_acc.append(np.mean(y_batch == y_pred))\n",
        "\n",
        "test_accuracy = np.mean(test_batch_acc)\n",
        "\n",
        "print(\"Final results:\")\n",
        "print(\"  test accuracy:\\t\\t{:.2f} %\".format(\n",
        "    test_accuracy * 100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "smSxMoh71KF6",
        "outputId": "820a06cf-8896-4b2d-a1d8-f29b87e10db3"
      },
      "execution_count": 241,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final results:\n",
            "  test accuracy:\t\t10.00 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lwQM0saYTpy9"
      },
      "source": [
        "__Hint:__ If you don't want to compute shapes by hand, just plug in any shape (e.g. 1 unit) and run compute_loss. You will see something like this:\n",
        "\n",
        "__`RuntimeError: size mismatch, m1: [5 x 1960], m2: [1 x 64] at /some/long/path/to/torch/operation`__\n",
        "\n",
        "See the __1960__ there? That's your actual input shape.\n",
        "\n",
        "## Task 2: adding normalization\n",
        "\n",
        "* Add batch norm (with default params) between convolution and ReLU\n",
        "  * nn.BatchNorm*d (1d for dense, 2d for conv)\n",
        "  * usually better to put them after linear/conv but before nonlinearity\n",
        "* Re-train the network with the same optimizer, it should get at least 60% validation accuracy at peak.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = nn.Sequential(\n",
        "    nn.Conv2d(3, 32, kernel_size=(3,3)),\n",
        "    nn.BatchNorm2d(32),\n",
        "    nn.ReLU(),\n",
        "    nn.MaxPool2d((2, 2)),\n",
        "    nn.Flatten(),\n",
        "    nn.Linear(32 * 15 * 15, 100),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(0.1),\n",
        "    nn.Linear(in_features=100, out_features=10)\n",
        ").to(device)"
      ],
      "metadata": {
        "id": "q6Wftnea5T8B"
      },
      "execution_count": 242,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = nn.Sequential(\n",
        "    nn.Conv2d(3, 32, kernel_size=(3,3), padding=1),\n",
        "    nn.BatchNorm2d(32),\n",
        "    nn.ReLU(),\n",
        "    nn.Conv2d(32, 64, kernel_size=(3,3), padding=1),\n",
        "    nn.BatchNorm2d(64),\n",
        "    nn.MaxPool2d((2, 2)),\n",
        "    nn.LeakyReLU(0.1),\n",
        "    nn.Conv2d(64, 128, kernel_size=(3,3), padding=1),\n",
        "    nn.BatchNorm2d(128),\n",
        "    nn.ReLU(),\n",
        "    nn.Conv2d(128, 256, kernel_size=(3,3), padding=1),\n",
        "    nn.BatchNorm2d(256),\n",
        "    nn.MaxPool2d((2, 2)),\n",
        "    nn.LeakyReLU(0.1),\n",
        "    nn.Conv2d(256, 512, kernel_size=(3,3), padding=1),\n",
        "    nn.BatchNorm2d(512),\n",
        "    nn.MaxPool2d((2, 2)),\n",
        "    nn.ReLU(),\n",
        "    nn.Flatten(),\n",
        "    nn.Linear(512 * 4 * 4, 256),\n",
        "    nn.LeakyReLU(0.1),\n",
        "    nn.Dropout(0.25),\n",
        "    nn.Linear(256, 128),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(0.25),\n",
        "    nn.Linear(128, 10)\n",
        ").to(device)"
      ],
      "metadata": {
        "id": "ZgVyuVIQLWDi"
      },
      "execution_count": 243,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "opt = torch.optim.Adam(model.parameters())\n",
        "\n",
        "train_loss = []\n",
        "val_accuracy = []\n",
        "\n",
        "num_epochs = 100 # total amount of full passes over training data\n",
        "batch_size = 50  # number of samples processed in one SGD iteration\n",
        "\n",
        "best_accuracy = -1.0\n",
        "best_epoch = -1\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # In each epoch, we do a full pass over the training data:\n",
        "    start_time = time.time()\n",
        "    model.train(True) # enable dropout / batch_norm training behavior\n",
        "    for X_batch, y_batch in iterate_minibatches(X_train, y_train, batch_size):\n",
        "        # train on batch\n",
        "        loss = compute_loss(X_batch, y_batch)\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "        opt.zero_grad()\n",
        "        train_loss.append(loss.item())  # .item() = convert 1-value Tensor to float\n",
        "\n",
        "    # And a full pass over the validation data:\n",
        "    model.train(False)     # disable dropout / use averages for batch_norm\n",
        "    with torch.no_grad():  # do not store intermediate activations\n",
        "        epoch_val_accuracy = [] # Calculate accuracy for the current epoch's validation pass\n",
        "        for X_batch, y_batch in iterate_minibatches(X_val, y_val, batch_size):\n",
        "            logits = model(torch.as_tensor(X_batch, dtype=torch.float32, device=device))\n",
        "            y_pred = logits.argmax(-1).detach().to(\"cpu\").numpy()\n",
        "            epoch_val_accuracy.append(np.mean(y_batch == y_pred))\n",
        "\n",
        "        current_accuracy = np.mean(epoch_val_accuracy) # Mean accuracy for the current epoch\n",
        "        val_accuracy.append(current_accuracy) # Append epoch accuracy to the list\n",
        "\n",
        "\n",
        "    # Then we print the results for this epoch:\n",
        "    print(\"Epoch {} of {} took {:.3f}s\".format(\n",
        "        epoch + 1, num_epochs, time.time() - start_time))\n",
        "    print(\"  training loss (in-iteration): \\t{:.6f}\".format(\n",
        "        np.mean(train_loss[-len(X_train) // batch_size :])))\n",
        "    print(\"  validation accuracy: \\t\\t\\t{:.2f} %\".format(\n",
        "        current_accuracy * 100)) # Use current_accuracy here\n",
        "\n",
        "    # Early stopping logic\n",
        "    if current_accuracy > best_accuracy:\n",
        "      best_accuracy = current_accuracy\n",
        "      best_epoch = epoch\n",
        "      torch.save(model.state_dict(), \"best_state.pt\")\n",
        "    elif epoch - best_epoch > 10:\n",
        "      print(f\"  Validation accuracy has not improved for 10 epochs. Stopping early at epoch {epoch + 1}.\")\n",
        "      break # early stopping\n",
        "\n",
        "\n",
        "# Load the best model state\n",
        "if os.path.exists(\"best_state.pt\"):\n",
        "    model.load_state_dict(torch.load(\"best_state.pt\"))\n",
        "    print(\"Loaded best model state.\")\n",
        "\n",
        "\n",
        "model.train(False) # disable dropout / use averages for batch_norm\n",
        "test_batch_acc = []\n",
        "with torch.no_grad():\n",
        "    for X_batch, y_batch in iterate_minibatches(X_test, y_test, 500):\n",
        "        logits = model(torch.as_tensor(X_batch, dtype=torch.float32, device=device))\n",
        "        y_pred = logits.max(1)[1].detach().cpu().numpy()\n",
        "        test_batch_acc.append(np.mean(y_batch == y_pred))\n",
        "\n",
        "test_accuracy = np.mean(test_batch_acc)\n",
        "\n",
        "print(\"Final results:\")\n",
        "print(\"  test accuracy:\\t\\t{:.2f} %\".format(\n",
        "    test_accuracy * 100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "QmTW7BvTz9Rx",
        "outputId": "88f589ba-a6f4-404d-f656-563ce1381a3a"
      },
      "execution_count": 244,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 of 100 took 12.634s\n",
            "  training loss (in-iteration): \t1.556915\n",
            "  validation accuracy: \t\t\t56.37 %\n",
            "Epoch 2 of 100 took 12.612s\n",
            "  training loss (in-iteration): \t1.103406\n",
            "  validation accuracy: \t\t\t64.16 %\n",
            "Epoch 3 of 100 took 12.681s\n",
            "  training loss (in-iteration): \t0.878950\n",
            "  validation accuracy: \t\t\t72.51 %\n",
            "Epoch 4 of 100 took 12.725s\n",
            "  training loss (in-iteration): \t0.730524\n",
            "  validation accuracy: \t\t\t75.87 %\n",
            "Epoch 5 of 100 took 12.389s\n",
            "  training loss (in-iteration): \t0.622831\n",
            "  validation accuracy: \t\t\t76.32 %\n",
            "Epoch 6 of 100 took 12.318s\n",
            "  training loss (in-iteration): \t0.519368\n",
            "  validation accuracy: \t\t\t79.01 %\n",
            "Epoch 7 of 100 took 12.319s\n",
            "  training loss (in-iteration): \t0.435477\n",
            "  validation accuracy: \t\t\t81.66 %\n",
            "Epoch 8 of 100 took 12.359s\n",
            "  training loss (in-iteration): \t0.357722\n",
            "  validation accuracy: \t\t\t79.79 %\n",
            "Epoch 9 of 100 took 12.417s\n",
            "  training loss (in-iteration): \t0.290713\n",
            "  validation accuracy: \t\t\t81.06 %\n",
            "Epoch 10 of 100 took 12.504s\n",
            "  training loss (in-iteration): \t0.236557\n",
            "  validation accuracy: \t\t\t83.87 %\n",
            "Epoch 11 of 100 took 12.397s\n",
            "  training loss (in-iteration): \t0.187850\n",
            "  validation accuracy: \t\t\t81.38 %\n",
            "Epoch 12 of 100 took 12.376s\n",
            "  training loss (in-iteration): \t0.148081\n",
            "  validation accuracy: \t\t\t83.13 %\n",
            "Epoch 13 of 100 took 12.378s\n",
            "  training loss (in-iteration): \t0.126832\n",
            "  validation accuracy: \t\t\t82.46 %\n",
            "Epoch 14 of 100 took 12.378s\n",
            "  training loss (in-iteration): \t0.102982\n",
            "  validation accuracy: \t\t\t83.50 %\n",
            "Epoch 15 of 100 took 12.381s\n",
            "  training loss (in-iteration): \t0.090701\n",
            "  validation accuracy: \t\t\t83.13 %\n",
            "Epoch 16 of 100 took 12.389s\n",
            "  training loss (in-iteration): \t0.081693\n",
            "  validation accuracy: \t\t\t83.55 %\n",
            "Epoch 17 of 100 took 12.391s\n",
            "  training loss (in-iteration): \t0.069938\n",
            "  validation accuracy: \t\t\t82.32 %\n",
            "Epoch 18 of 100 took 12.404s\n",
            "  training loss (in-iteration): \t0.068291\n",
            "  validation accuracy: \t\t\t84.12 %\n",
            "Epoch 19 of 100 took 12.389s\n",
            "  training loss (in-iteration): \t0.058141\n",
            "  validation accuracy: \t\t\t83.21 %\n",
            "Epoch 20 of 100 took 12.383s\n",
            "  training loss (in-iteration): \t0.061131\n",
            "  validation accuracy: \t\t\t82.02 %\n",
            "Epoch 21 of 100 took 12.377s\n",
            "  training loss (in-iteration): \t0.057969\n",
            "  validation accuracy: \t\t\t82.74 %\n",
            "Epoch 22 of 100 took 12.369s\n",
            "  training loss (in-iteration): \t0.051448\n",
            "  validation accuracy: \t\t\t82.41 %\n",
            "Epoch 23 of 100 took 12.377s\n",
            "  training loss (in-iteration): \t0.048036\n",
            "  validation accuracy: \t\t\t82.88 %\n",
            "Epoch 24 of 100 took 12.374s\n",
            "  training loss (in-iteration): \t0.053057\n",
            "  validation accuracy: \t\t\t81.41 %\n",
            "Epoch 25 of 100 took 12.368s\n",
            "  training loss (in-iteration): \t0.045411\n",
            "  validation accuracy: \t\t\t83.67 %\n",
            "Epoch 26 of 100 took 12.364s\n",
            "  training loss (in-iteration): \t0.043356\n",
            "  validation accuracy: \t\t\t83.25 %\n",
            "Epoch 27 of 100 took 12.362s\n",
            "  training loss (in-iteration): \t0.040070\n",
            "  validation accuracy: \t\t\t84.35 %\n",
            "Epoch 28 of 100 took 12.353s\n",
            "  training loss (in-iteration): \t0.040442\n",
            "  validation accuracy: \t\t\t83.62 %\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-310828531.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mX_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterate_minibatches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;31m# train on batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3379513173.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(X_batch, y_batch)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mX_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0my_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    242\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    245\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1776\u001b[0m     \u001b[0;31m# fmt: off\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1778\u001b[0;31m         \u001b[0mforward_call\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1779\u001b[0m         \u001b[0;31m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1780\u001b[0m         \u001b[0;31m# this function, and just call forward.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RB1f9ebCTpy9"
      },
      "source": [
        "## Task 3: Data Augmentation\n",
        "\n",
        "There's a powerful torch tool for image preprocessing useful to do data preprocessing and augmentation.\n",
        "\n",
        "Here's how it works: we define a pipeline that\n",
        "* makes random crops of data (augmentation)\n",
        "* randomly flips image horizontally (augmentation)\n",
        "* then normalizes it (preprocessing)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "d8-OSeicTpy-"
      },
      "outputs": [],
      "source": [
        "from torchvision import transforms\n",
        "means = np.array((0.4914, 0.4822, 0.4465))  # statistics from dataset documentation\n",
        "stds = np.array((0.2023, 0.1994, 0.2010))\n",
        "\n",
        "transform_augment = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomRotation([-30, 30]),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(means, stds),\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "a21LpvXxTpy-"
      },
      "outputs": [],
      "source": [
        "from torchvision.datasets import CIFAR10\n",
        "train_loader = CIFAR10(\"./cifar_data/\", train=True, transform=transform_augment)\n",
        "\n",
        "train_dataloader = torch.utils.data.DataLoader(\n",
        "    train_loader,  batch_size=32, shuffle=True, num_workers=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "FZfWqT8pTpy-"
      },
      "outputs": [],
      "source": [
        "# for (x_batch, y_batch) in train_dataloader:\n",
        "\n",
        "#     print('X:', type(x_batch), x_batch.shape)\n",
        "#     print('y:', type(y_batch), y_batch.shape)\n",
        "\n",
        "#     for i, img in enumerate(x_batch.numpy()[:8]):\n",
        "#         plt.subplot(2, 4, i+1)\n",
        "#         plt.imshow(img.transpose([1,2,0]) * stds + means )\n",
        "\n",
        "\n",
        "#     raise NotImplementedError(\"Plese use this code in your training loop\")\n",
        "    # TODO use this in your training loop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kv2pU1HJTpy_"
      },
      "source": [
        "When testing, we don't need random crops, just normalize with same statistics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "HgS33Q-5Tpy_"
      },
      "outputs": [],
      "source": [
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(means, stds),\n",
        "])\n",
        "\n",
        "test_loader = CIFAR10(\"./cifar_data/\", train=False, transform=transform_test)\n",
        "test_dataloader = torch.utils.data.DataLoader(\n",
        "    test_loader,  batch_size=32, shuffle=False, num_workers=1)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "43489768"
      },
      "source": [
        "opt = torch.optim.Adam(model.parameters())\n",
        "\n",
        "train_loss = []\n",
        "val_accuracy = []\n",
        "\n",
        "num_epochs = 0\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # In each epoch, we do a full pass over the training data:\n",
        "    start_time = time.time()\n",
        "    model.train(True) # enable dropout / batch_norm training behavior\n",
        "    for X_batch, y_batch in train_dataloader: # Use train_dataloader\n",
        "        # move data to device\n",
        "        X_batch = X_batch.to(device)\n",
        "        y_batch = y_batch.to(device)\n",
        "\n",
        "        # train on batch\n",
        "        logits = model(torch.as_tensor(X_batch, dtype=torch.float32, device=device))\n",
        "        loss = F.cross_entropy(logits, y_batch)\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "        opt.zero_grad()\n",
        "        train_loss.append(loss.item())  # .item() = convert 1-value Tensor to float\n",
        "\n",
        "    # And a full pass over the validation data (using test_dataloader as validation here):\n",
        "    model.train(False)     # disable dropout / use averages for batch_norm\n",
        "    with torch.no_grad():  # do not store intermediate activations\n",
        "        val_batch_acc = []\n",
        "        for X_batch, y_batch in test_dataloader: # Use test_dataloader for validation\n",
        "            # move data to device\n",
        "            X_batch = X_batch.to(device)\n",
        "            y_batch = y_batch.to(device)\n",
        "\n",
        "            logits = model(torch.as_tensor(X_batch, dtype=torch.float32, device=device))\n",
        "            y_pred = logits.argmax(-1).detach().to(\"cpu\").numpy()\n",
        "            val_batch_acc.append(np.mean(y_batch.to(\"cpu\").numpy() == y_pred)) # Move y_batch to cpu for comparison\n",
        "\n",
        "        val_accuracy.append(np.mean(val_batch_acc))\n",
        "\n",
        "\n",
        "    # Then we print the results for this epoch:\n",
        "    print(\"Epoch {} of {} took {:.3f}s\".format(\n",
        "        epoch + 1, num_epochs, time.time() - start_time))\n",
        "    print(\"  training loss (in-iteration): \\t{:.6f}\".format(\n",
        "        np.mean(train_loss[-len(train_dataloader) :]))) # Adjusted to use len(train_dataloader)\n",
        "    print(\"  validation accuracy: \\t\\t\\t{:.2f} %\".format(\n",
        "        np.mean(val_accuracy[-1]) * 100))\n",
        "\n",
        "\n",
        "# Evaluate on the test set after training\n",
        "model.train(False) # disable dropout / use averages for batch_norm\n",
        "test_batch_acc = []\n",
        "with torch.no_grad():\n",
        "    for X_batch, y_batch in test_dataloader: # Use test_dataloader for final evaluation\n",
        "        X_batch = X_batch.to(device)\n",
        "        y_batch = y_batch.to(device)\n",
        "\n",
        "        logits = model(torch.as_tensor(X_batch, dtype=torch.float32, device=device))\n",
        "        y_pred = logits.max(1)[1].detach().to(\"cpu\").numpy()\n",
        "        test_batch_acc.append(np.mean(y_batch.to(\"cpu\").numpy() == y_pred)) # Move y_batch to cpu for comparison\n",
        "\n",
        "\n",
        "test_accuracy = np.mean(test_batch_acc)\n",
        "\n",
        "print(\"Final results:\")\n",
        "print(\"  test accuracy:\\t\\t{:.2f} %\".format(\n",
        "    test_accuracy * 100))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CUpNZCOMTpy_"
      },
      "source": [
        "# Homework 2.2: The Quest For A Better Network\n",
        "\n",
        "In this assignment you will build a monster network to solve CIFAR10 image classification.\n",
        "\n",
        "This notebook is intended as a sequel to seminar 3, please give it a try if you haven't done so yet."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OL6UHGwcTpy_"
      },
      "source": [
        "(please read it at least diagonally)\n",
        "\n",
        "* The ultimate quest is to create a network that has as high __accuracy__ as you can push it.\n",
        "* There is a __mini-report__ at the end that you will have to fill in. We recommend reading it first and filling it while you iterate.\n",
        "\n",
        "## Grading\n",
        "* starting at zero points\n",
        "* +20% for describing your iteration path in a report below.\n",
        "* +20% for building a network that gets above 20% accuracy\n",
        "* +10% for beating each of these milestones on __TEST__ dataset:\n",
        "    * 50% (50% points)\n",
        "    * 60% (60% points)\n",
        "    * 65% (70% points)\n",
        "    * 70% (80% points)\n",
        "    * 75% (90% points)\n",
        "    * 80% (full points)\n",
        "    \n",
        "## Restrictions\n",
        "* Please do NOT use pre-trained networks for this assignment until you reach 80%.\n",
        " * In other words, base milestones must be beaten without pre-trained nets (and such net must be present in the e-mail). After that, you can use whatever you want.\n",
        "* you __can__ use validation data for training, but you __can't'__ do anything with test data apart from running the evaluation procedure.\n",
        "\n",
        "## Tips on what can be done:\n",
        "\n",
        "\n",
        " * __Network size__\n",
        "   * MOAR neurons,\n",
        "   * MOAR layers, ([torch.nn docs](http://pytorch.org/docs/master/nn.html))\n",
        "\n",
        "   * Nonlinearities in the hidden layers\n",
        "     * tanh, relu, leaky relu, etc\n",
        "   * Larger networks may take more epochs to train, so don't discard your net just because it could didn't beat the baseline in 5 epochs.\n",
        "\n",
        "   * Ph'nglui mglw'nafh Cthulhu R'lyeh wgah'nagl fhtagn!\n",
        "\n",
        "\n",
        "### The main rule of prototyping: one change at a time\n",
        "   * By now you probably have several ideas on what to change. By all means, try them out! But there's a catch: __never test several new things at once__.\n",
        "\n",
        "\n",
        "### Optimization\n",
        "   * Training for 100 epochs regardless of anything is probably a bad idea.\n",
        "   * Some networks converge over 5 epochs, others - over 500.\n",
        "   * Way to go: stop when validation score is 10 iterations past maximum\n",
        "   * You should certainly use adaptive optimizers\n",
        "     * rmsprop, nesterov_momentum, adam, adagrad and so on.\n",
        "     * Converge faster and sometimes reach better optima\n",
        "     * It might make sense to tweak learning rate/momentum, other learning parameters, batch size and number of epochs\n",
        "   * __BatchNormalization__ (nn.BatchNorm2d) for the win!\n",
        "     * Sometimes more batch normalization is better.\n",
        "   * __Regularize__ to prevent overfitting\n",
        "     * Add some L2 weight norm to the loss function, PyTorch will do the rest\n",
        "       * Can be done manually or with weight_decay parameter of a optimizer ([for example SGD's doc](https://pytorch.org/docs/stable/optim.html#torch.optim.SGD)).\n",
        "     * Dropout (`nn.Dropout`) - to prevent overfitting\n",
        "       * Don't overdo it. Check if it actually makes your network better\n",
        "   \n",
        "### Convolution architectures\n",
        "   * This task __can__ be solved by a sequence of convolutions and poolings with batch_norm and ReLU seasoning, but you shouldn't necessarily stop there.\n",
        "   * [Inception family](https://hacktilldawn.com/2016/09/25/inception-modules-explained-and-implemented/), [ResNet family](https://towardsdatascience.com/an-overview-of-resnet-and-its-variants-5281e2f56035?gi=9018057983ca), [Densely-connected convolutions (exotic)](https://arxiv.org/abs/1608.06993), [Capsule networks (exotic)](https://arxiv.org/abs/1710.09829)\n",
        "   * Please do try a few simple architectures before you go for resnet-152.\n",
        "   * Warning! Training convolutional networks can take long without GPU. That's okay.\n",
        "     * If you are CPU-only, we still recomment that you try a simple convolutional architecture\n",
        "     * a perfect option is if you can set it up to run at nighttime and check it up at the morning.\n",
        "     * Make reasonable layer size estimates. A 128-neuron first convolution is likely an overkill.\n",
        "     * __To reduce computation__ time by a factor in exchange for some accuracy drop, try using __stride__ parameter. A stride=2 convolution should take roughly 1/4 of the default (stride=1) one.\n",
        "\n",
        "   \n",
        "### Data augmemntation\n",
        "   * getting 5x as large dataset for free is a great\n",
        "     * Zoom-in+slice = move\n",
        "     * Rotate+zoom(to remove black stripes)\n",
        "     * Add Noize (gaussian or bernoulli)\n",
        "   * Simple way to do that (if you have PIL/Image):\n",
        "     * ```from scipy.misc import imrotate,imresize```\n",
        "     * and a few slicing\n",
        "     * Other cool libraries: cv2, skimake, PIL/Pillow\n",
        "   * A more advanced way is to use torchvision transforms:\n",
        "    ```\n",
        "    transform_train = transforms.Compose([\n",
        "        transforms.RandomCrop(32, padding=4),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "    ])\n",
        "    trainset = torchvision.datasets.CIFAR10(root=path_to_cifar_like_in_seminar, train=True, download=True, transform=transform_train)\n",
        "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True, num_workers=2)\n",
        "\n",
        "    ```\n",
        "   * Or use this tool from Keras (requires theano/tensorflow): [tutorial](https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html), [docs](https://keras.io/preprocessing/image/)\n",
        "   * Stay realistic. There's usually no point in flipping dogs upside down as that is not the way you usually see them.\n",
        "   \n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}